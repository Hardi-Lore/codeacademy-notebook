{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTdgXvUwJugfO/E0zN8vJ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PySpark"],"metadata":{"id":"gzixlg-moSn-"}},{"cell_type":"markdown","source":["## Big Data for Data Engineers\n","Big data is all around us! Learn how to harness it here"],"metadata":{"id":"37enzHRTpGab"}},{"cell_type":"markdown","source":["\n","### What is PySpark?\n","PySpark is an API for Apache Spark. Apache Spark is an open-source framework for distributed computing. Distributed computing makes handling big data with a little (or medium) computer possible. And this unit will cover everything you need to know to get started managing big data.\n","\n","## What will Big Data with PySpark for Data Engineers cover?\n","- Conceptual foundations of big data\n","- Writing programs in PySpark\n","- Using SQL-style queries in PySpark\n","- Handing data with Resilient Distributed Datasets (RDDs)\n","- Handling data with PySpark DataFrames\n","- Generating summary reports from big data datasets\n"],"metadata":{"id":"K7Wx8YaEoYts"}},{"cell_type":"markdown","source":["## What is Big Data?\n","“big data” when referring to a dataset that was unmanageable with current business intelligence tools.\n","- Sources of data have continued to grow\n","- Outpacing the growth of computing power\n","\n","After the massive growth of the internet (mid-2000s). Many analysts in the industry were struggling to handle their own data.\n","\n","- Big data: Greater than our available computing power can handle\n","- How we can describe it with the 3 Vs (volume, velocity, variety)\n","- Applications of big data\n"],"metadata":{"id":"WtgKSJDDouEw"}},{"cell_type":"markdown","source":["### Data is Everywhere\n","We generate data from all kinds of activities:\n","\n","    *   whether a transaction at a local store\n","    *   website we visit\n","    *   the location of our cell phone\n","    *   tweets are written on Twitter\n","\n","\n","\n","- At the time of this writing, an average of 500 million tweets are written on Twitter each day. A person at Twitter can’t analyze this data! Data of this size is often referred to as big data.\n","\n","- Big data is any data that is too big for a typical modern computer to process and analyze.\n","\n","- However, the definition of big data is relative to the amount of computing power we have available.\n","\n","\n","For example:\n","\n","- Most current personal computers have somewhere between 8-32 GB of random access memory (RAM) available for data processing. That means, from the perspective of a personal computer, any dataset larger than 10-20 GB might be too large to process.\n","\n","- A large enterprise can take advantage of larger computing resources (i.e., a warehouse of servers or cloud computing ), so 100+ GB might be the upper limit for the size of data.\n","\n","- Data measured in terabytes (1 TB = 1000 GB) is the largest amount of data being worked with at this time .\n","\n","\n","\n"],"metadata":{"id":"SFI1oJKmo95x"}},{"cell_type":"markdown","source":["### The 3 Vs\n","Define big data using the features that make it hard to handle in the first place. We can generally categorize big data by what are known as the three Vs:\n","\n","    *   Volume\n","\n","Big data is “big”.\n","“big” the data is bigger than the amount of available computing power. Zettabytes of data are created every year ( zettabyte is 1 billion terabytes).\n","\n","    *   Velocity\n","\n","Big data has velocity, it is growing quickly.\n","Through means like apps and sensors, data becomes faster, cheaper, and easier to collect automatically and continuously.\n","If data were simply large, but slow-changing, then over time our computing power would eventually catch up to the size of the data.\n","\n","    *   Variety\n","\n","Big data also has variety, it comes in different, complex, forms.\n","In today’s data ecosystem, data comes in many more formats than the data tables of old. Data can be categorized as:\n","- structured (data tables with rows and columns)\n","- semi-structured (think JSON files with nested data)\n","- unstructured (audio, image, and video data).\n","\n","Each of these data formats presents different challenges in processing.\n","**bold text**"],"metadata":{"id":"pgrBY8aDqMse"}},{"cell_type":"markdown","source":["### Big Data Applications\n","What can we do with big data?\n","\n","When do we run into big data in the real world? Let’s explore a few examples of big data applications across various industries.\n","\n","    *   Social Media\n","With an average of 500 million tweets per day, Twitter data would definitely qualify as big data. Despite the massive amount of data at its disposal, Twitter provides analytics for each user with the ability to dive into historical Tweet activity and identify trends. In order to provide this for each Twitter user, they must be using some kind of big data toolkit to store and analyze the data.\n","\n","    *   Healthcare\n","If we look at the healthcare industry, a rising trend that many providers want to enable is known as evidence-based medicine. Healthcare providers want to combine data from several sources, including cell phone apps, diagnostic tests, and previous medical records, to give recommendations for each patient. Providers hope this will avoid expensive and unnecessary tests and improve patient outcomes. In this case, there is a lot of data from many different sources and formats, but the efforts could provide a huge impact for their patients.\n","\n","    *   Finance\n","In the financial industry, credit card companies aim to reduce the amount of fraudulent transactions, as these cost money and cause hardship for their customers. Using different tools, credit card companies are able to analyze every single credit card transaction and use machine learning models to identify transactions that could be fraudulent. This saves a massive amount of money for themselves and for their customers!\n","\n","    *   Final Thoughts\n","No matter which industry we look at, we will find numerous examples of big data everywhere, and there are more every day. However, we need to be aware that big data often comes with both big challenges and big effects. With the right tools and techniques, we can begin to extract value from big data while being mindful of both its limitations and impacts.\n"],"metadata":{"id":"ryO4kaqXvSzh"}},{"cell_type":"markdown","source":["## Bias in Data\n","Biases are systematic errors in thinking influenced by cultural and personal experiences.\n","\n","    *   Biases distort our perception and cause us to make incorrect decisions.\n","    *   Humans have many biases, both implicit and explicit.\n","    \n","To understand the different types of bias that show up at each stage of analyzing data. Types of bias that impact data analysis and data-driven decision-making.\n"],"metadata":{"id":"BleXKg71vvQZ"}},{"cell_type":"markdown","source":["    1.   Automation bias\n","\n","Automation bias stems from the idea that computers or machines are\n","\n","              *   more trustworthy than humans\n","              *   more objective than humans\n","\n","Automation bias is at the root of why people follow their GPS into trouble, even when contradictory information is available.\n","Computers, data, and algorithms are not actually completely objective:\n","\n","- Human biases can be encoded into the algorithms.\n","- Look at more information sources when we evaluate data analysis results or reports.\n","- data analysis not immune to bias\n","\n","Pay attention to other information streams (our eyes and ears) when we drive with GPS.\n","\n","\n","    2.   Bias in building and optimizing algorithms\n","\n","Algorithmic bias arises when an algorithm produces systematic and repeatable errors that lead to unfair outcomes, such as privileging one group over another.\n","\n","Algorithmic bias can be initiated through selection bias and then reinforced and perpetuated by other bias types.\n","\n","    3.   Bias in interpreting results and drawing conclusions\n","Bias also influences the final stages of data analysis: interpreting results and drawing conclusions. The following bias types are ones we should watch out for when evaluating or generating data reports:\n","\n","- Confirmation bias is our tendency to seek out information that supports our views.\n","\n","    *   influences data analysis when we consciously or unconsciously interpret results in a way that supports our original hypothesis.\n","    \n","    *   To limit confirmation bias, clearly state hypotheses and goals before starting an analysis, and then honestly evaluate how they influenced our interpretation and reporting of results.\n","\n","- Overgeneralization bias is inappropriately extending observations made with one dataset to other datasets, leading to overinterpreting results and unjustified extrapolation. To limit overgeneralization bias, be thoughtful when interpreting data, only extend results beyond the dataset used to generate them when it is justified, and only extend results to the proper population.\n","\n","- Reporting bias is the human tendency to only report or share results that affirm our beliefs or hypotheses, also known as “positive” results. Editors, publishers, and readers are also subject to reporting bias as positive results are published, read, and cited more often. To limit reporting bias, report negative results and cite others who do, too.\n","\n","       Conclusions\n","Data and machine learning algorithms are now ubiquitous. They influence decisions about\n","       - Who is hired or fired,\n","       - Who accepted into schools\n","       - Who allowed to rent houses\n","       - Influence which neighborhoods are more heavily policed\n","       - Who is granted parole.\n","\n","Therefore, we must recognize that data and algorithms can be biased, just like the humans who create and train them. Learning more about the types of bias that influence how algorithms function will improve our ability to perform and interpret data analyses and will help us make more informed decisions.\n"],"metadata":{"id":"SUQlJ2dCwflD"}},{"cell_type":"markdown","source":["## Big Data Storage and Computing\n","Learn about the challenges of storing and analyzing big data\n"],"metadata":{"id":"xv84fpBK0DtV"}},{"cell_type":"markdown","source":["#### Big Data Challenges\n","Every single day, over 2.510^18 bytes of data are created:\n","\n","         * transactional sales data\n","         * Internet of Things (IoT) devices\n","\n","… data sources grow in both size and velocity at a rapid rate. When thinking about the massive scale of data.\n","\n","\n","1. Where are all of these data stored?\n","\n","Basic dataset as a table in Excel or an equivalent application. we pull an entire dataset into RAM on a single processing machine for computation and either crash or take too long to process, making analysis impossible.\n","\n","2. How do we get enough computing power to process it?\n","\n"],"metadata":{"id":"-NbVDuHZ0Qrc"}},{"cell_type":"markdown","source":["#### Big Data Storage\n","* A popular solution for big datasets is a distributed file system on a network of hardware called a cluster.\n","\n","* A cluster is a group of several machines called nodes, with a cluster manager node and multiple worker nodes.\n","\n","![Big Data Storage](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Big%20Data%20Storage.png)\n","\n","\n","- The cluster manager manages resources and sends commands to the worker nodes that store the data.\n","- Data saved on worker nodes are replicated multiple times for fault tolerance.\n","\n","    *   This allows access to the complete dataset even in the event that one of the worker nodes goes offline.\n","\n","    *   This type of file storage system is also easily and infinitely scalable, as additional worker nodes can be added indefinitely.\n"],"metadata":{"id":"GjzoPTbJ1QD9"}},{"cell_type":"markdown","source":["##### Hadoop Distributed File System (HDFS)\n","\n","    Cluster node <- Cluster Manager <- MapReduce framework <- HDFS store data <- Apache\n","\n","1. MapReduce is a computing framework for analyzing datasets housed on a distributed file system like HDFS. **MapReduce is a disk-oriented method**, which means\n","2. MapReduce writes data to disk in intermediate steps of analysis. While this method allows us to process data stored on the HDFS, it can still be a slow process for analysing larger datasets.\n","\n","Framework for a cluster system is called Hadoop Distributed File System (HDFS), which is part of a set of tools distributed by Apache.\n","\n","- HDFS was designed to store vast amounts of data to be processed using another framework called MapReduce.\n","\n","- HDFS requires a specific hardware configuration that can be a costly barrier to entry. For this reason, cloud-hosted HDFS is a popular fix.\n","\n","    *   Microsoft Azure\n","    *   Amazon Web Services (AWS)\n","\n","  … offer cloud-based HDFS solutions, allowing companies to outsource a system’s setup and hardware management for a fixed monthly cost.\n","\n","\n","\n","\n","1. HDFS solutions both store and process data on each worker node,\n","  - they ensure that we have enough computing power to tackle our data problems.\n","1. When data grows in size, our number of nodes may be increased to add more storage and computing power.\n","1. This is advantageous for scaling but can become expensive as the number of nodes increases.\n","\n","MapReduce was the standard for big data processing for a while, but over time it could not keep up with the rate at which data were growing and changing.\n","* The map function collects specifically defined elements of data from each node as key-value tuple pairs.\n","* The reduce function is an analytical function applied to each key-value pair dataset whose solution is returned as output.\n","\n","Apache Spark is a better alternative for processing. Spark’s main benefit was the ability to process data in the node’s memory instead of processing on disk as MapReduce does. This provided much better performance and unlocked new capabilities for working with big data.\n","\n","    Cluster node <- MapReduce framework <- Cluster Manager <- HDFS store data <- Apache\n","\n","\n","In order to get value out of big data, we need to utilize the best strategies for storing our data and providing computing power for our analysis. With these in place, we can be ready to scale and grow our analyses!\n","\n"],"metadata":{"id":"cbmxv9w460s1"}},{"cell_type":"markdown","source":["### Questions\n","\n","\n","1. MapReduce is a framework for big data computing developed to extract and analyze data across a Hadoop Distributed File System (HDFS) cluster.\n","\n","MapReduce for big data computing is performed simultaneously on each worker node before being sent back to the manager node, allowing for a much faster compute time.\n","\n","\n","2. Big data may be described by its extremely large volume, which is relative to our modern computing power.\n","\n","We use velocity to describe big data because it is always growing and outpacing our computing power. Big data comes in a variety of forms, like structured and unstructured.\n","\n","\n","3. Hadoop Distributed File System (HDFS)  is implemented on a cluster of computers: a cluster manager and several worker nodes to house the data and computing resources together.\n","\n","\n","4. Which statement most accurately describes big data?\n","\n","Big data refers to data that is too large to handle with our current computing power and is relative to the system’s total available Random Access Memory (RAM).\n","Data is “big” when it is bigger than we can handle, and is relative to total RAM size.\n","\n"],"metadata":{"id":"TETZsf_r_5XK"}},{"cell_type":"markdown","source":["## What is Spark?\n","Learn about Apache Spark and its application for big data analysis\n","A popular choice is the Hadoop Distributed File System (HDFS), which splits up a dataset and stores it across multiple worker nodes in a cluster.\n","\n","MapReduce is a computing framework for analyzing datasets housed on a distributed file system like HDFS. MapReduce is a disk-oriented method, which means:\n","\n","- MapReduce writes data to disk in intermediate steps of analysis. While this method allows us to process data stored on the HDFS, it can still be a slow process for analyzing larger datasets.\n","\n","PySpark is an API developed to minimize this learning obstacle by allowing programmers to write Python syntax to build Spark applications. There are also APIs for Java and R."],"metadata":{"id":"oBdV9pO5AhFt"}},{"cell_type":"markdown","source":["### Apache Spark and PySpark\n","As big data processing needs have grown, Spark is an analytics engine:\n","\n","    *   originally developed at UC Berkeley\n","    *   donated to the open-sourced Apache Software Foundation.\n","    \n","Spark was designed as a solution for\n","\n","    *   processing big datasets\n","    *   was specifically developed to build data pipelines for machine learning applications.\n","\n","MapReduce, Spark does not have its own file storage system and is designed to be used with distributed file systems like HDFS. Spark can also be run on a single node (single computer) in stand-alone mode with a non-distributed dataset.\n","\n","![Apache Spark and PySpark](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Apache_Spark_and_PySpark.png)\n","\n","- Spark uses the RAM of each cluster node in unison, harnessing the power of multiple computers.\n","- Spark applications execute analyses up to 100 times faster than MapReduce because\n","\n","    *   Spark caches data and intermediate tables in RAM.\n","    *   However, as datasets become larger, the advantage of using RAM decreases and can disappear altogether.\n"],"metadata":{"id":"vi6LKZPFAsw-"}},{"cell_type":"markdown","source":["### How Spark Works\n","\n","- The Spark driver is used to create a Spark session\n","    *   The Spark driver is the entry point of a Spark application.\n","- The driver program communicates with the cluster manager to create resilient distributed datasets (RDDs).\n","    *   To create an RDD, the data is divided up and distributed across worker nodes in a cluster.\n","    *   Copies of the RDD across the nodes ensure that RDDs are fault-tolerant, so information is recoverable in the event of a failure.\n","- Two types of operations can be performed on RDDs:\n","\n","  1. Transformations manipulate RDDs on the cluster.\n","  2. Actions return a computation back to the main driver program.\n","\n","![How_Spark_Works](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/How_Spark_Works.png)\n","\n","\n","The cluster manager determines the resources that the Spark application requires and assigns a specific number of worker nodes as executors to handle the processing of RDDs.\n","\n","Spark can be run on top of cluster managers including:\n","\n","    *  Hadoop’s YARN\n","    *  Apache Mesos."],"metadata":{"id":"bCV5x7fkDVCi"}},{"cell_type":"markdown","source":["### Spark Modules\n","The driver program is the core of the Spark application, but there are also modules that have been developed to enhance the utility of Spark. These modules include:\n","\n","* Spark SQL: an API that converts SQL queries and actions into Spark tasks to be distributed by the cluster manager. This allows for the integration of existing SQL pipelines without redevelopment of code and subsequent testing required for quality control.\n","* Spark Streaming: a solution for processing live data streams that creates a discretized stream (Dstream) of RDD batches.\n","* MLlib and ML: machine learning modules for designing pipelines used for feature engineering and algorithm training. ML is the DataFrame-based improvement on the original MLlib module.\n","* GraphX: a robust graphing solution for Spark. More than just visualizing data, this API converts RDDs to resilient distributed property graphs (RDPGs) which utilize vertex and edge properties for relational data analysis.\n"],"metadata":{"id":"kHHJk0qqDWSF"}},{"cell_type":"markdown","source":["### Spark Limitations\n","Spark is a powerful tool for working with big data, but it does come with some limitations:\n","\n","Expensive hardware requirements: Spark provides a solution for more time-efficient analyses of large distributed datasets, but Spark analyses are much less cost-effective. The costs associated with Spark come from the need for a lot of RAM built into the worker nodes of a cluster. RAM is much more expensive than disk memory.\n","\n","**Real-time processing is not possible**: Spark Streaming offers near real-time data processing, but true real-time data analysis is not supported.\n","\n","**Manual optimization is required**: The benefits and power of Spark must be optimized by the developer, which requires an advanced understanding of the program and backend, creating a technical hurdle for developers."],"metadata":{"id":"xpCw-lHgDXiJ"}},{"cell_type":"markdown","source":["### Cases\n","TripAdvisor is an online travel site that sources, generates, and analyzes massive amounts of data per day. All of the data processing for TripAdvisor is done using Spark. Natural language processing of reviews is an example shared by TripAdvisor in an article on their site. https://www.tripadvisor.com/engineering/using-apache-spark-for-massively-parallel-nlp/\n","\n","MyFitnessPal is a popular application for smartwatches and smartphones owned by Under Armour that tracks the diet and exercise of its users. The app utilizes Spark to analyze user data for their users and internal marketing demographic classification. You can read more about how MyFitnessPal uses Spark in this article by the Wall Street Journal. https://www.wsj.com/articles/BL-CIOB-7254\n","\n","We can find many other examples of Spark use cases for commercial business and research projects in recent years.\n","There are other big data analysis solutions that are built using Spark code or using similar concepts. Some examples include:\n","- Delta Lake\n","- Apache Mesos\n","- Rumble (Apache)\n","- DataBricks\n","\n"],"metadata":{"id":"H8tTCLA_DXQw"}},{"cell_type":"markdown","source":["## RDDS WITH PYSPARK"],"metadata":{"id":"oXIno2OM-FP1"}},{"cell_type":"markdown","source":["### Start Coding with PySpark\n","\n","SparkSession is the entry point to Spark. There are many possible configurations for a SparkSession, but for now, we will simply start a new session and save it as spark:\n"],"metadata":{"id":"tFNVIFsJ-GQM"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()"],"metadata":{"id":"qFtCYVYU-o3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use Spark with data stored on :\n","- a distributed file system (DFS or HDFS)\n","- your local machine (PC)\n"],"metadata":{"id":"GZhjkQq--sTq"}},{"cell_type":"markdown","source":["\n","Without additional configurations, Spark defaults to local with the number of partitions set to the number of CPU cores on our local machine (often, this is 4)."],"metadata":{"id":"qu3AeBeMA47l"}},{"cell_type":"code","source":["print(spark)\n","#Output: <pyspark.sql.session.SparkSession object at 0x7f4330b163d0>"],"metadata":{"id":"kp-O2i5kA9DM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Within a SparkSession:"],"metadata":{"id":"Rsz54vMiBBZq"}},{"cell_type":"code","source":["# default setting\n","rdd_par = spark.sparkContext.parallelize(dataset_name)\n"],"metadata":{"id":"NME_7eriBCC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- `sparkContext` is the connection to the cluster and gives us the ability to create and transform RDDs.\n","- `parallelize()` function can create an RDD from data saved locally.\n","\n","  - We can specify the number of partitions, which is generally recommended as 2-4 partitions per machine.\n","  - Spark defaults to the total number of CPU cores (4)."],"metadata":{"id":"SJwzauTxBHDV"}},{"cell_type":"code","source":[" # with partition argument of 10\n"," rdd_txt = spark.sparkContext.textFile(\"file_name.txt\", 10)"],"metadata":{"id":"hVd6KUcsCi44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we are working with an:\n","- external dataset\n","- possibly a large dataset stored on a distributed file system,\n","\n","… we can use `textFile()` to create an RDD.\n","\n"," Spark’s default is to partition the text file in 128 MB blocks, but we can manually set the number of partitions within the function.\n","\n"],"metadata":{"id":"2tU39_CIClUT"}},{"cell_type":"markdown","source":["\n","We can verify the number of partitions in rdd_txt using the following line:"],"metadata":{"id":"AhDck5KxCzcg"}},{"cell_type":"code","source":["dd_txt.getNumPartitions()\n","# output: 10\n","\n"],"metadata":{"id":"PafR30X8C6Vc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we need to know how to end our SparkSession when we are finished with our work:\n"],"metadata":{"id":"8Ir8A4_FC7vr"}},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"Py_MInmwC9Db"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformations\n"],"metadata":{"id":"gbHa76cB-d7I"}},{"cell_type":"markdown","source":["Each function took an RDD as input and returned an RDD as output. In Spark, functions with this behavior are called transformations. You can find more transformations in the official Spark documentation.\n","\n","**A transformation performed on an RDD will always produce an RDD.**\n","\n","https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n","\n","- Many Spark functions we use on RDDs ≈ functions used in Python\n","- Use lambda expressions within RDD functions.\n","\n","Example:  lambda expression that adds the number 1 to its input."],"metadata":{"id":"dsqELu-tDBMG"}},{"cell_type":"code","source":["add_one = lambda x: x+1 # apply x+1 to x\n","print(add_one(10)) # this will output 11"],"metadata":{"id":"VO_0t__kDTxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PySpark functions that we may already be familiar with:\n","\n","`map()` applies an operation to each element of the RDD, so it’s often constructed with a lambda expression.\n","\n","This map example adds 1 to each element in our RDD:\n","\n"],"metadata":{"id":"QcnmBm-4DXQl"}},{"cell_type":"code","source":["rdd = spark.SparkContent.parallelize([1,2,3,4,5])\n","rdd.map(lambda x: x+1)\n","# output RDD [2,3,4,5,6]"],"metadata":{"id":"Z3ex-Bn7DcQW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**RDD contains tuples**, we can map the lambda expression to the elements with a specific index value. The following code maps the lambda expression to just the first element of each tuple but keeps the others in the output:\n","\n"],"metadata":{"id":"Rl7I2UuTDjI9"}},{"cell_type":"code","source":["# input RDD [(1,2,3),(4,5,6),(7,8,9)]\n","rdd.map(lambda x: (x[0]+1, x[1], x[2]))\n","# output RDD [(2,2,3),(5,5,6),(8,8,9)]"],"metadata":{"id":"hfGB578PDmS6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`filter()` allows us to remove or keep data conditionally. If we want to remove all NULL values in the following RDD, we can use a lambda expression in our filter:\n","\n"],"metadata":{"id":"AmWhqTCcDnGs"}},{"cell_type":"code","source":["# input RDD [1,2,NULL,4,5]\n","rdd.filter(lambda x: x is not None)\n","# output RDD [1,2,4,5]"],"metadata":{"id":"7loHGkQkDsSK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have one final note about transformations:\n","\n","We can only view the contents of an RDD by using a special function like collect(), which will **print the data stored in the RDD**. So to view the new RDD in the previous example, we would run the following:\n","\n"],"metadata":{"id":"nALHnLy2Dvh4"}},{"cell_type":"code","source":["rdd.filter(lambda x: x is not None).collect()\n","# Output: [1,2,4,5]"],"metadata":{"id":"wNnrNutbDxMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question\n","from pyspark.sql import SparkSession\n","student_data = [(\"Chris\",1523,0.72,\"CA\"),  (\"Jake\", 1555,0.83,\"NY\"),  (\"Cody\", 1439,0.92,\"CA\"),\n","               (\"Lisa\",1442,0.81,\"FL\"), (\"Daniel\",1600,0.88,\"TX\"),  (\"Kelvin\",1382,0.99,\"FL\"),\n","               (\"Nancy\",1442,0.74,\"TX\"),  (\"Pavel\",1599,0.82,\"NY\"), (\"Josh\",1482,0.78,\"CA\"),\n","               (\"Cynthia\",1582,0.94,\"CA\")]\n","spark = SparkSession.builder.getOrCreate()\n","student_rdd = spark.sparkContext.parallelize(student_data)\n","\n","\n"],"metadata":{"id":"wYAbehoxD6zZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n","rdd_transformation.collect()\n","\"\"\"Output:\n","[('Chris', 1523, 72, 'CA'), ('Jake', 1555, 83, 'NY'), ('Cody', 1439, 92, 'CA'),\n","('Lisa', 1442, 81, 'FL'), ('Daniel', 1600, 88, 'TX'), ('Kelvin', 1382, 99, 'FL'),\n","('Nancy', 1442, 74, 'TX'), ('Pavel', 1599, 82, 'NY'), ('Josh', 1482, 78, 'CA'),\n","('Cynthia', 1582, 94, 'CA')]\n","\"\"\"\n","\n","\n"],"metadata":{"id":"DNNCfG8kEDEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdd_filtered = rdd_transformation.filter(lambda x: x[2]>80)\n","rdd_filtered.collect()\n","\"\"\"Output:\n","[('Jake', 1555, 83, 'NY'), ('Cody', 1439, 92, 'CA'), ('Lisa', 1442, 81, 'FL'),\n","('Daniel', 1600, 88, 'TX'), ('Kelvin', 1382, 99, 'FL'), ('Pavel', 1599, 82, 'NY'),\n","('Cynthia', 1582, 94, 'CA')]\n","\"\"\""],"metadata":{"id":"xWDXAWodERv8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Actions\n"],"metadata":{"id":"Vz_SUBb_-f9h"}},{"cell_type":"markdown","source":["- `collect()`\n","- `take(n)`\n","- `reduce()`\n","\n","Spark executes transformations only when an action is called to return a value. This delay is why we call Spark transformations lazy.\n","\n","Spark will queue up the transformations to optimize and reduce overhead once an action is called. Let’s say that we wanted to apply a map and filter to our RDD:"],"metadata":{"id":"jYmA4EN_GheY"}},{"cell_type":"code","source":["rdd = spark.SparkContent.parallelize([1,2,3,4,5])\n","rdd.map(lambda x: x+1).filter(lambda x: x>3)\n","rdd.collect()"],"metadata":{"id":"DEyZjSpTG8hB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of following the order that we called the transformations,\n","- First - spark might load the values greater than 3 into memory\n","- Second - perform the map function last\n","- Third - transformations executed only when the action collect() was called to return the entire contents of the new RDD as a list\n","\n","This swap will save memory and time because Spark loaded fewer data points and mapped the lambda to fewer elements."],"metadata":{"id":"hQcNEFxzG9MJ"}},{"cell_type":"markdown","source":["Use `take(n)` to view the first n elements of a large RDD"],"metadata":{"id":"thXlFtmvHLU2"}},{"cell_type":"code","source":["# input RDD [1,2,3,4,5]\n","rdd.take(3)\n","# returned RDD [1, 2, 3]\n"],"metadata":{"id":"cnS4teHZHMdt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use the action `reduce()` to return fewer elements of our RDD by applying certain operators.\n","Example, sum all the values in the RDD. We can use reduce() with a lambda to add each element sequentially.\n","\n"],"metadata":{"id":"dEqfrMFNHQ-p"}},{"cell_type":"code","source":["# input RDD [1,2,3,4,5]\n","rdd.reduce(lambda x,y: x+y)\n","# returned RDD 15"],"metadata":{"id":"kGh5PvD9HSsW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`reduce()` is powerful because it is used to apply many arbitrary operations to an RDD\n"],"metadata":{"id":"-UvekDIyHb-5"}},{"cell_type":"code","source":["# Example:\n","sum_gpa = rdd_transformation.map(lambda x: x[2]).reduce(lambda x,y: x+y)\n","# view the sum\n","sum_gpa\n","# output: 843\n","\n","sum_gpa / rdd_transformation.count()\n","# 84.3\n"],"metadata":{"id":"eIvF8FfXHgdY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Associative and Commutative Properties\n"],"metadata":{"id":"5B6393UH-hez"}},{"cell_type":"markdown","source":["The `reduce(expression)` function we used previously is a powerful aggregation tool, but  limitations to the operations it can apply to RDDs.\n","\n","`Expression` must be commutative and associative due to the nature of parallelized computation\n","- commutative the output is independent of the order in which tasks complete\n","- associative the output is independent on how the data is grouped\n","\n","Spark operates in parallel — tasks that have commutative and associative properties allow for parallelization.\n","- The commutative property allows for all parallel tasks to execute and conclude without waiting for another task to complete.\n","- The associative property allows Spark to partition and distribute our data to multiple nodes because the result will stay the same no matter how tasks are grouped.\n","\n","Let’s try to break that down a bit further with math! No matter how you switch up or break down summations, they’ll always have the same result thanks to the commutative and associative properties:\n","\n","1+2+3+4+5 = (3+4+5)+(1+2) = (4)+(2+5+1)+(3) = 15\n","However, this is not the case with division:\n","\n","1÷2÷3÷4÷5 ≠ (3÷4÷5)÷(1÷2) ≠ 4÷(2÷5÷1)÷3\n"],"metadata":{"id":"2EagiLmaHp0f"}},{"cell_type":"markdown","source":["![Big Data Storage](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Associative%20and%20Commutative%20Properties.png)\n"],"metadata":{"id":"KnNFf8IeKVVi"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate() # SparkSession started and end it at the end spark.stop()\n","\n"],"metadata":{"id":"bD35YJk4KQuX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the very handy transformation glom() we can print out how our data is partitioned and the resulting summation and division of the partitions.\n"],"metadata":{"id":"ObJnc3WaKdPc"}},{"cell_type":"code","source":["data = [1,2,3,4,5]\n","for i in range(1,5):\n","   rdd = spark.sparkContext.parallelize(data, i)\n","   print('partition: ', rdd.glom().collect())\n","   print('addition: ', rdd.reduce(lambda a,b: a+b))\n","\n","\n","# partition:  [[1, 2, 3, 4, 5]]\n","# sum:  15\n","# partition:  [[1, 2], [3, 4, 5]]\n","# sum:  15\n","# partition:  [[1], [2, 3], [4, 5]]\n","# sum:  15\n","# partition:  [[1], [2], [3], [4, 5]]\n","# sum:  15\n"],"metadata":{"id":"cGX2hM53KhAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(1,5):\n","   rdd = spark.sparkContext.parallelize(data, i)\n","   print('partition: ', rdd.glom().collect())\n","   print('division: ', rdd.reduce(lambda a,b: a/b))\n","\n","# partition:  [[1, 2, 3, 4, 5]]\n","# division:  0.008333333333333333\n","# partition:  [[1, 2], [3, 4, 5]]\n","# division:  3.3333333333333335\n","# partition:  [[1], [2, 3], [4, 5]]\n","# division:  1.875\n","# partition:  [[1], [2], [3], [4, 5]]\n","# division:  0.20833333333333331\n"],"metadata":{"id":"G3ELvsfJKjIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"VfZDhPHGKgko"}},{"cell_type":"markdown","source":["### Shared variables"],"metadata":{"id":"beWJ9zas-iw0"}},{"cell_type":"markdown","source":["#### Broadcast Variables\n","\n","Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks.\n","Broadcast variables are cached  - PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.(also known as shared variables)\n","Never want to broadcast large amounts of data because the size would be too much to serialize and send through the network.\n"],"metadata":{"id":"5QkSxtsBLKtA"}},{"cell_type":"code","source":["# Example\n","# list of states\n","states = ['FL', 'NY', 'TX', 'CA', 'NY', 'NY', 'FL', 'TX']\n","# convert to RDD\n","states_rdd = spark.sparkContext.parallelize(states)"],"metadata":{"id":"prwdb_MxLRSW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- We want the region instead of the state, such as “East” or “South”.\n","- RDD is partitioned in the Spark cluster, and we don’t know which nodes contain data on which states.\n","\n","We need to send the conversion information to all nodes because it’s very likely that each node will contain multiple distinct states.\n","We can provide each node with information on which states belong in each region. This information, Spark calls broadcast variables.\n"],"metadata":{"id":"_bYuovPLLT2a"}},{"cell_type":"markdown","source":["Creating a conversion dictionary called region that matches each state to its region:\n"],"metadata":{"id":"1PyavNetdNam"}},{"cell_type":"code","source":["# dictionary of regions\n","region = {\"NY\":\"East\", \"CA\":\"West\", \"TX\":\"South\", \"FL\":\"South\"}"],"metadata":{"id":"uSeeBhfMLs30"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We can then broadcast our region dictionary and apply the conversion to each element in the RDD with our map function:"],"metadata":{"id":"owoozC_mLrJo"}},{"cell_type":"code","source":["# broadcast region dictionary to nodes\n","broadcast_var = spark.sparkContext.broadcast(region)\n","# map regions to states\n","result = states_rdd.map(lambda x: broadcast_var.value[x])\n","# view first four results\n","result.take(4)\n","# output : [‘South’, ‘East’, ‘South’, ‘West’]\n"],"metadata":{"id":"qhAkJk0zLwG6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is Spark’s efficient method of sharing variables amongst its nodes (also known as shared variables)."],"metadata":{"id":"dXBrlhgsLyfO"}},{"cell_type":"markdown","source":["Question:\n"],"metadata":{"id":"vHzl2e2eLzkG"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","student_data = [(\"Chris\",1523,0.72,\"CA\"), (\"Jake\", 1555,0.83,\"NY\"), (\"Cody\", 1439,0.92,\"CA\"),\n","               (\"Lisa\",1442,0.81,\"FL\"), (\"Daniel\",1600,0.88,\"TX\"), (\"Kelvin\",1382,0.99,\"FL\"),\n","               (\"Nancy\",1442,0.74,\"TX\"), (\"Pavel\",1599,0.82,\"NY\"), (\"Josh\",1482,0.78,\"CA\"),\n","               (\"Cynthia\",1582,0.94,\"CA\")]\n","student_rdd = spark.sparkContext.parallelize(student_data)\n","rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n","states = {\"NY\":\"New York\", \"CA\":\"California\", \"TX\":\"Texas\", \"FL\":\"Florida\"}\n","\n","\n","\n","## YOUR SOLUTION HERE ##\n","broadcastStates = spark.sparkContext.broadcast(states)\n","# confirm type\n","type(broadcastStates) # pyspark.broadcast.Broadcast\n","\n","\n","\n","## YOUR SOLUTION HERE ##\n","rdd_broadcast = rdd_transformation.map(lambda x: (x[0],x[1],x[2],broadcastStates.value[x[3]]))\n","# confirm transformation is correct\n","rdd_broadcast.collect()\n","\"\"\"# [('Chris', 1523, 72, 'California'), ('Jake', 1555, 83, 'New York'), ('Cody', 1439, 92, 'California'),\n","#  ('Lisa', 1442, 81, 'Florida'), ('Daniel', 1600, 88, 'Texas'),  ('Kelvin', 1382, 99, 'Florida'),\n","#  ('Nancy', 1442, 74, 'Texas'),  ('Pavel', 1599, 82, 'New York'),  ('Josh', 1482, 78, 'California'),\n","#  ('Cynthia', 1582, 94, 'California')]\"\"\"\n","spark.stop()"],"metadata":{"id":"-GRwaQG6L2qF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Accumulator Variables\n","\n","Accumulator variables can be updated and are primarily used as counters or sums. Conceptually, they’re similar to the sum and count functions in NumPy.\n","- They can keep track of the inputs and outputs of each Spark task by aggregating the size of each subsequent transformation.\n","- monitor for data loss - we could count the number of NULL values or the resulting size of each transformation.\n","- It’s best to avoid using accumulators in transformations. Whenever Spark runs into an exception, it will re-execute the tasks. This will incorrectly increment the accumulator. However, Spark will guarantee that this does not happen to accumulators in actions.\n","\n","How many “East” versus “West” entries there are.\n","\n","- We could attempt to create a couple of variables to keep track of the counts, but we might run into serialization and overhead issues when datasets get really big.\n","- Let’s see how we can implement accumulator variables by counting the number of distinct regions. Since this will be a new dataset, let’s create an RDD first\n"],"metadata":{"id":"LQntNELPL90s"}},{"cell_type":"code","source":["region = ['East', 'East', 'West', 'South', 'West', 'East', 'East', 'West', 'North']\n","rdd = spark.sparkContext.parallelize(region)"],"metadata":{"id":"QwsyaQF5L9Xm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We’ll start off by initializing the accumulator variables at zero:"],"metadata":{"id":"0BXOIW9XAHoD"}},{"cell_type":"code","source":["east = spark.sparkContext.accumulator(0)\n","west = spark.sparkContext.accumulator(0)"],"metadata":{"id":"snNdfwqVAIE1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let’s create a function to increment each accumulator by one whenever Spark encounters ‘East’ or ‘West’:\n"],"metadata":{"id":"Wf3mYJhBANXA"}},{"cell_type":"code","source":["def countCoasts(r):\n","   if 'East' in r: east.add(1)\n","   elif 'West' in r: west.add(1)"],"metadata":{"id":"lqfxQ9qLAPv4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We’ll take the function we created and run it against each element in the RDD."],"metadata":{"id":"gWYKxsrfAVBh"}},{"cell_type":"code","source":["rdd.foreach(lambda x: countCoasts(x))\n","print(east) # output: 4\n","print(west) # output: 3"],"metadata":{"id":"PHrckTdxAW8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question:\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","student_data = [(\"Chris\",1523,0.72,\"CA\"),\n","               (\"Jake\", 1555,0.83,\"NY\"),\n","               (\"Cody\", 1439,0.92,\"CA\"),\n","               (\"Lisa\",1442,0.81,\"FL\"),\n","               (\"Daniel\",1600,0.88,\"TX\"),\n","               (\"Kelvin\",1382,0.99,\"FL\"),\n","               (\"Nancy\",1442,0.74,\"TX\"),\n","               (\"Pavel\",1599,0.82,\"NY\"),\n","               (\"Josh\",1482,0.78,\"CA\"),\n","               (\"Cynthia\",1582,0.94,\"CA\")]\n","student_rdd = spark.sparkContext.parallelize(student_data)\n","rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], int(x[2]*100), x[3]))\n","states = {\"NY\":\"New York\", \"CA\":\"California\", \"TX\":\"Texas\", \"FL\":\"Florida\"}\n","broadcastStates = spark.sparkContext.broadcast(states)\n","rdd_broadcast = rdd_transformation.map(lambda x: (x[0],x[1],x[2],broadcastStates.value[x[3]]))\n","\n","\n","\n","\n","\n","\n","## YOUR SOLUTION HERE ##\n","sat_1500 = spark.sparkContext.accumulator(0)\n","\n","\n","# confirm type\n","type(sat_1500) # pyspark.accumulators.Accumulator\n","\n","\n","\n","\n","## YOUR SOLUTION HERE ##\n","def count_high_sat_score(x):\n","   if x > 1500 : sat_1500.add(1)\n","\n","\n","# confirm saved as a function\n","print(count_high_sat_score) # <function count_high_sat_score at 0x7fde70198280>\n","\n","\n","\n","\n","## YOUR SOLUTION HERE ##\n","rdd_broadcast.foreach(lambda x:count_high_sat_score(x[1]))\n","\n","\n","# confirm accumulator worked\n","print(sat_1500) # 5\n"],"metadata":{"id":"1Pk6wJSFAbUm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Review\n","Congratulations! You’ve just finished your first coding adventure with PySpark! In this lesson, we learned that:\n","- RDDs are the foundational data structure of Spark\n","- RDDs are fault-tolerant, partitioned, and operated on in parallel\n","- Transformations are lazy and do not execute until an action is called\n","\n","We also learned how to:\n","- Transform and summarize RDDs with transformations and actions\n","- Send information to all nodes with broadcast variables\n","- Debug work with accumulator variables\n","\n","\n","![Apache Spark and PySpark](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/RDDS%20WITH%20PYSPARK.png)"],"metadata":{"id":"y00geSSrAqHc"}},{"cell_type":"markdown","source":["Suppose we had 2 Spark RDDs we would like to join. One of these RDDs should be broadcasted to each node before transforming it with `.join()`. Given the following information, which one should be broadcasted?"],"metadata":{"id":"nfDTKDk4A2oS"}},{"cell_type":"code","source":["print(rdd1.count()) # 100 tuples\n","print(rdd2.count()) # 1,050,000,000 tuples\n","\n","\n","broadcasted_rdd = broadcast(rdd1)\n","joined_rdd = rdd2.join(broadcasted_rdd)\n","\n"],"metadata":{"id":"W7kJAKXiA-7d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What are the three properties of RDDs?\n","- Resilient\n","- Distributed\n","- Parallelized\n","\n","RDDs are fault-tolerant (resilient), distributed throughout multiple nodes, and are operated on in parallel."],"metadata":{"id":"bGeKVzzGBBPm"}},{"cell_type":"markdown","source":["## PYSPARK SQL\n","\n"],"metadata":{"id":"8j6pep1XDd9C"}},{"cell_type":"markdown","source":["### Introducing PySpark SQL\n","\n","- we may not always want to perform complicated analysis directly on RDDs\n","- Spark SQL that can make common data analysis tasks simpler and faster.\n","\n","The name Spark SQL is an umbrella term, as there are several ways to interact with data when using this module\n","\n","\n","\n","1. Basic inspection and querying data in a Spark DataFrame.\n","2. Same operations using standard SQL directly in our PySpark code.\n","\n","* SparkSession -> sparkContext -> entry point to Spark SQL\n","* Start a SparkSession, is a wrapper around a sparkContext and contains all the metadata required to start working with distributed data.\n","* `SparkSession.builder` to set configuration parameters and create a new session"],"metadata":{"id":"GL7CXmt0D1Kq"}},{"cell_type":"code","source":["spark = SparkSession.builder.config('spark.app.name', 'learning_spark_sql').getOrCreate()\n"],"metadata":{"id":"b0ENNDqIEufg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["set one configuration parameter `spark.app.name`\n"," call the `.getOrCreate()` method to initialize the new `SparkSession`"],"metadata":{"id":"XyNEX2sJEjUg"}},{"cell_type":"code","source":["print(spark.sparkContext)\n","# <SparkContext master=local[*] appName=learning_spark_sql>"],"metadata":{"id":"mmvar3YaE1dV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can access the `SparkContext` for a session with `SparkSession.sparkContext`\n","\n","- Use the SparkSession to create DataFrames, read external files, register tables, and run SQL queries over saved data.\n","- Terminate the session with SparkSession.stop()\n","  - finished the analysis\n","  - clear the Spark cache\n","- Now that we’re familiar with the basics of SparkSession, the next step is to begin using Spark SQL to interact with data\n","- DataFrames can be created manually from RDDs using rdd.toDF(`[\"names\", \"of\", \"columns\"]`).\n"],"metadata":{"id":"kGy5Zm3lFDYS"}},{"cell_type":"code","source":["# Create an RDD from a list\n","hrly_views_rdd  = spark.sparkContext.parallelize([\n","   \t\t\t\t\t[\"Betty_White\" , 288886],\n","   \t\t\t\t\t[\"Main_Page\", 139564],\n","   \t\t\t\t\t[\"New_Year's_Day\", 7892],\n","   \t\t\t\t\t[\"ABBA\", 8154]\n"," \t\t\t\t\t\t\t])\n","# Convert RDD to DataFrame\n","hrly_views_df = hrly_views_rdd .toDF([\"article_title\", \"view_count\"])"],"metadata":{"id":"5ms0eOslFDBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use the DataFrame.show(n_rows) method to print the first n_rows of a Spark DataFrame.  truncate=False to ensure all columns are visible.\n","\n"],"metadata":{"id":"Z3iY__y8HGq8"}},{"cell_type":"code","source":["hrly_views_df.show(4, truncate=False)"],"metadata":{"id":"vI8zRMuUHImJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![Apache Spark and PySpark](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/PySpark%20SQL.png)\n","\n","Access the underlying RDD with `DataFrame.rdd` , the underlying metadata.\n","Keep in mind that a DataFrame is a structure built on top of an RDD. When we check the type of hrly_views_df_rdd, we can see that it’s an RDD!\n"],"metadata":{"id":"Xb4p04y5HbT7"}},{"cell_type":"code","source":["# Access DataFrame's underlying RDD\n","hrly_views_df_rdd = hrly_views_df.rdd\n","# Check object type\n","print(type(hrly_views_df_rdd))\n","# <class 'pyspark.rdd.RDD'>"],"metadata":{"id":"w16lNawsIr0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Spark DataFrames from External Sources\n","how to pull in larger datasets from external sources\n","\n"],"metadata":{"id":"-dYJsK1YIlkY"}},{"cell_type":"code","source":["print(type(spark.read))\n","# <class 'pyspark.sql.readwriter.DataFrameReader'>\n","\n","\n","# Read CSV to DataFrame\n","hrly_views_df = spark.read.option('header', True) .option('delimiter', ' ') \\\n"," \t\t\t\t\t\t     .option('inferSchema', True)\\\n"," \t\t\t\t\t\t      .csv('views_2022_01_01_000000.csv')"],"metadata":{"id":"ZCkopgtGI6nW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are a few things going on in this code, let’s go through them one at a time:\n","- This code uses the `SparkSession.read` function to create a new `DataFrameReader`\n","- The `DataFrameReader` has an `.option`('`option_name`', '`option_value`')method that can be used to instruct Spark how to read a file.\n","\n","In this case, we used the following options:\n","\n","- `.option`('`header`', `True`)  — Indicate the file already contains a header row. By default, Spark assumes there is no header.\n","- `.option`('`delimiter`', '` `')— Indicates each column is separated by a space (‘ ‘). By default, Spark assumes CSV columns are separated by commas.\n","- `.option`('`inferSchema`', `True`) — Instructs Spark to sample a subset of rows before determining each column’s type. By default, Spark will treat all CSV columns as strings.\n","\n","The `DataFrameReader` also has a `.csv('path')` method which loads a CSV file and returns the result as a DataFrame.\n","\n","There are a few quick ways of checking that our data has been read in properly. The most direct way is checking DataFrame.show().\n"],"metadata":{"id":"PgIn_xpLJDgP"}},{"cell_type":"code","source":["# Display first 5 rows of DataFrame\n","hrly_views_df.show(5, truncate=False)"],"metadata":{"id":"F9o2xMpRKMvi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![Apache Spark and PySpark](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Spark%20DataFrames%20from%20External%20Sources.png)\n","\n","we used a DataFrameReader to pull a CSV from disk into our local Spark environment. However, Spark can read a wide variety of file formats. You can refer to the PySpark documentation to explore all available DataFrameReader options and file formats. In the following exercise, we’ll start to analyze the contents of this file."],"metadata":{"id":"rTNBsBZ6KSkk"}},{"cell_type":"markdown","source":["### Inspecting and Cleaning Data With PySpark\n"],"metadata":{"id":"uRFNyi4FLp6v"}},{"cell_type":"markdown","source":["how Spark can help with data exploration and data analysis.\n","Like Pandas, Spark DataFrames offer a series of operations for:\n","- cleaning data\n","- inspecting data\n","- transforming data\n","\n","All DataFrames have a schema that defines their structure, columns, and datatypes. We can use `DataFrame.printSchema()` to show a DataFrame’s schema."],"metadata":{"id":"GoIkQseBLuJa"}},{"cell_type":"code","source":["# Display DataFrame schema\n","hrly_views_df.printSchema()\n","\"\"\"output:\n","root\n","|-- language_code: string (nullable = true)\n","|-- article_title: string (nullable = true)\n","|-- hourly_count: intereger (nullable = true)\n","|-- monthly_count: intereger (nullable = true)\n","\"\"\""],"metadata":{"id":"N_A1KNGlL5LK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- `DataFrame.describe()` to see a high-level summary of the data by column.\n","- Result is a DataFrame in itself, so we append `.show()` to get it to display in our notebook.\n"],"metadata":{"id":"bUsjYm3sL58O"}},{"cell_type":"code","source":["hrly_views_df_desc = hrly_views_df.describe()\n","hrly_views_df_desc.show(truncate=False)"],"metadata":{"id":"yOybmdoyNB1b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![Apache Spark and PySpark](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Spark%20DataFrames%20from%20External%20Sources-1.png)\n","\n","From this summary, we can see.\n","- About 4.65 million unique pages were visited this hour\n","- The most visited page had almost 289,000 visitors, while the mean page had just over 4.5 visitors.\n","\n","`DataFrame.drop(\"columns\", \"to\", \"drop\")` can drop this columns by name"],"metadata":{"id":"KgtBSJGMO84D"}},{"cell_type":"code","source":["# Drop `monthly_count` and display new DataFrame\n","hrly_views_df = hrly_views_df.drop('monthly_count')\n","hrly_views_df.show(5)"],"metadata":{"id":"Y40f3EOuO8oO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![Apache Spark and PySpark](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Spark%20DataFrames%20from%20External%20Sources-2.png)\n","\n","`DataFrame.withColumnRenamed('old_name', 'new_name')`    \n","We can replace this misleading header with a better name."],"metadata":{"id":"cAh7zDlkRHhb"}},{"cell_type":"code","source":["hrly_views_df = hrly_views_df.withColumnRenamed('article_title', 'page_title')\n","hrly_views_df.printSchema()"],"metadata":{"id":"GD2ep-_GRR5y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![Apache Spark and PySpark](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Spark%20DataFrames%20from%20External%20Sources-3.png)\n","\n","- Spark assigned all columns `nullable = true`\n","- DataFrameReader reads a CSV, it assigns `nullable = true` to all columns.\n","- Intuitively, we know that `article_title` shouldn’t be null"],"metadata":{"id":"alAMN24IRVxc"}},{"cell_type":"markdown","source":["### Querying PySpark DataFrames\n","\n","- Performing analysis–with PySpark SQL .\n","- PySpark SQL DataFrames has built-in methods that can help with analyzing data.\n","\n","Some methods for analysis:\n","- `.filter()` method  is similar to SQL “WHERE” clause.\n","- `.select()` is used to choose which columns to return in our result.\n","  - `DataFrame.select([\"A\", \"B\", \"C\"])` similar to `SELECT A, B, C FROM DataFrame` in SQL.\n","\n","- `.orderBy()` is similar to SQL’s ORDER BY in SQL.\n","  - `.orderBy('hourly_count', ascending=False) `to specify the sort column and order logic."],"metadata":{"id":"zWeft026R-k2"}},{"cell_type":"markdown","source":["Examples\n","To filter our data to pages from a specific Wikipedia `language_code` (e.g., \"`kw`.m\")"],"metadata":{"id":"7Z4xthw_SlpO"}},{"cell_type":"code","source":[" hrly_views_df.filter(hrly_views_df.language_code == \"kw.m\").show(truncate=False)\n"],"metadata":{"id":"f-vKtn2yXa9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![https://github.com/Hardi-Lore/codeacademy-notebook/tree/main/pictures](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Querying%20PySpark%20DataFrames.png)"],"metadata":{"id":"_0uFjxN9XbaP"}},{"cell_type":"markdown","source":["- remove the monthly_count column ()\n","- display the data ordered by the hourly_count\n"],"metadata":{"id":"toqTdIXmX53L"}},{"cell_type":"code","source":["hrly_views_df.filter(hrly_views_df.language_code == \"kw.m\")\\\n","   \t\t.select(['language_code', 'article_title', 'hourly_count'])\\\n","   \t\t.orderBy('hourly_count', ascending=False)\\\n","   \t\t.show(5, truncate=False)\n"],"metadata":{"id":"_GLAajn4X_ec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![https://github.com/Hardi-Lore/codeacademy-notebook/tree/main/pictures](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Querying%20PySpark%20DataFrames-1.png)\n","\n","\n","- Select the sum of hourly_count by language_code\n","- Which sites were most active this hour\n"],"metadata":{"id":"EAFnNzpzYrcY"}},{"cell_type":"code","source":[" hrly_views_df.select(['language_code', 'hourly_count'])\\\n","   \t\t.groupBy('language_code')\\\n","  \t\t.sum() \\\n","   \t\t.orderBy('sum(hourly_count)', ascending=False)\\\n","   \t\t.show(5, truncate=False)"],"metadata":{"id":"hD5wNhiKY0gO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["![https://github.com/Hardi-Lore/codeacademy-notebook/tree/main/pictures](https://raw.githubusercontent.com/Hardi-Lore/codeacademy-notebook/main/pictures/Querying%20PySpark%20DataFrames-2.png)\n","\n","\n","- `.groupBy('language_code').sum()` to calculate the sum of all columns grouped by `language_code`,\n","- This code also orders our results with `.orderBy`(, using the name of the constructed column, '`sum(hourly_count)`'."],"metadata":{"id":"PIMhuPFUZDf2"}},{"cell_type":"markdown","source":["#### Querying PySpark with SQL\n","\n","SparkSession.sql()method we can analyze data in Spark with standard SQL\n","- PySpark DataFrame’s query methods are an improvement on performing analysis directly on RDDs.  (requires some practice)\n","\n","Before querying a DataFrame with SQL in Spark, it must be saved to the SparkSession’s catalog."],"metadata":{"id":"57qgcs08Zhfe"}},{"cell_type":"code","source":["hrly_views_df.createOrReplaceTempView('hourly_counts')\n"],"metadata":{"id":"QCICG-JBZvTZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- DataFrame as a local temporary view in memory.\n","- As long as the current SparkSession is active, we can use SparkSession.sql() to query it.\n","\n","Each of the three sections of SQL below performs the same function as the DataFrame query methods described in the previous exercise.\n","\n","\n"],"metadata":{"id":"UxRFIpvBZxjg"}},{"cell_type":"code","source":["# Pyspark SQL\n","query = \"\"\"SELECT * FROM hourly_counts WHERE language_code = 'kw.m'\"\"\"\n","spark.sql(query).show(truncate=False)\n","\n","# Pyspark\n","hrly_views_df.filter(hrly_views_df.language_code == \"kw.m\")\\\n","                                                                    .show(truncate=False)\n","\n","# Pyspark SQL\n","query = \"\"\"SELECT language_code, article_title, hourly_count\n","   FROM hourly_counts\n","   WHERE language_code = 'kw.m'\n","   ORDER BY hourly_count DESC\"\"\"\n","spark.sql(query).show(truncate=False)\n","\n","# Pyspark\n","hrly_views_df.filter(hrly_views_df.language_code == \"kw.m\")\\\n","           .select(['language_code', 'article_title', 'hourly_count'])\\\n","   \t\t.orderBy('hourly_count', ascending=False)\\\n","   \t\t.show(5, truncate=False)\n","\n","\n","# Pyspark SQL\n","query = \"\"\"SELECT language_code, SUM(hourly_count) as sum_hourly_count\n","   FROM hourly_counts\n","   GROUP BY language_code\n","   ORDER BY sum_hourly_count DESC\"\"\"\n","spark.sql(query).show(5, truncate=False)\n","\n","# Pyspark\n","hrly_views_df.select(['language_code', 'hourly_count'])\\\n","   \t\t.groupBy('language_code')\\\n","  \t\t.sum() \\\n","   \t\t.orderBy('sum(hourly_count)', ascending=False)\\\n","   \t\t.show(5, truncate=False)\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"rsFhtNaZaZZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Saving PySpark DataFrames\n","\n","\n","Once you’ve done some analysis, the next step is often saving the transformed data back to disk for others to use. In this final topic, we’re going to cover how to efficiently save PySpark DataFrames.\n","\n","- Spark offers a `.write.csv()` method. The Dataset is saved to disk."],"metadata":{"id":"CtuKXMcna0AT"}},{"cell_type":"code","source":[" hrly_views_df.write.csv('cleaned/csv/views_2022_01_01_000000/', mode=\"overwrite\")"],"metadata":{"id":"XdFOdAo-bIgV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- To write DataFrames to a directory of files rather than a single CSV file.\n","  - Spark runs all operations in parallel\n","- `.csv(“path/to/directory”,mode=\"overwrite\").write.parquet( )`\n","- `.csv()` does not retain information about its format/schema\n","- `.write.parquet( )` preserves information about a dataset’s schema."],"metadata":{"id":"mH1fs2-la8o3"}},{"cell_type":"code","source":[" hrly_views_slim_df .write.parquet('cleaned/parquet/views_2022_01_01_000000/', mode=\"overwrite\")\n"],"metadata":{"id":"6Lwj-DiybL_a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Question\n","1.\n","- PySpark DataFrames must have a defined column and row schema.\n","- In PySpark, a DataFrame is a distributed and built on top of Spark’s resilient distributed datasets (RDDs).\n","- PySpark DataFrames are built on top of resilient distributed datasets (RDDs).\n","- PySpark DataFrames are distributed collections of data.\n","\n","2. DataFrame.createOrReplaceTempView() can be used to save a DataFrame or query result in memory for future analysis.\n","\n","3.\n","- Spark can parallelize operations on DataFrames regardless of the file format they were created from. However, operations on parquet files are often faster than on CSV files.\n","- Parquet files are efficiently compressed and thus smaller than CSV files containing the same data.\n","- Parquet files preserve information about a DataFrame’s schema.\n","- Performing analysis on parquet files is often faster than CSV files.\n","\n"],"metadata":{"id":"AFRNhpC3bfUO"}},{"cell_type":"markdown","source":["# let's Learn to impliment Pyspark"],"metadata":{"id":"0JfIQes4Jz6V"}},{"cell_type":"markdown","source":["## Introducing PySpark SQL"],"metadata":{"id":"HQihO0htKFCu"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession\\\n","    .builder\\\n","    .master('local[*]')\\\n","    .config('spark.driver.memory', '1g')\\\n","    .config('spark.app.name', 'learning_spark_sql')\\\n","    .getOrCreate()\n","\n","# The variable `spark` is a `pyspark.sql.session.SparkSession`\n","type(spark)\n","\n","\n"],"metadata":{"id":"rX0Ct8PVKG4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The underlying `SparkContext` can be accessed from the `SparkSession`\n","type(spark.sparkContext)\n"],"metadata":{"id":"Y3wTEuxsKWOZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"s842G89-KX75"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating Spark DataFrames"],"metadata":{"id":"wmdkn14FKfxC"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a new SparkSession\n","spark = SparkSession\\\n","    .builder\\\n","    .config('spark.app.name', 'learning_spark_sql')\\\n","    .getOrCreate()\n","\n","sample_page_views  = spark.sparkContext.parallelize([\n","    [\"en\", \"Statue_of_Liberty\", \"2022-01-01\", 263],\n","    [\"en\", \"Replicas_of_the_Statue_of_Liberty\", \"2022-01-01\", 11],\n","    [\"en\", \"Statue_of_Lucille_Ball\" ,\"2022-01-01\", 6],\n","    [\"en\", \"Statue_of_Liberty_National_Monument\", \"2022-01-01\", 4],\n","    [\"en\", \"Statue_of_Liberty_play\"  ,\"2022-01-01\", 3],\n","])"],"metadata":{"id":"mb2SCFriKhx-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Create a DataFrame from `sample_page_views`."],"metadata":{"id":"XQPA0wqXPEeF"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","sample_page_views_df = sample_page_views.toDF([\"language_code\", \"title\", \"date\", \"count\"])\n","\n","# show first 5 rows\n","sample_page_views_df.show(5, truncate=False)"],"metadata":{"id":"wtLyoYrjPGXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Access the RDD underlying `sample_page_views_df`."],"metadata":{"id":"2ABtoG5NPIwK"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","sample_page_views_rdd_restored = sample_page_views_df.rdd\n","\n","# show restored RDD\n","sample_page_views_rdd_restored.collect()"],"metadata":{"id":"yjiBqGNlPLEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"tcsWoLBFPMoE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spark DataFrames from External Sources"],"metadata":{"id":"3OQtNTerPKgO"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a new SparkSession\n","spark = SparkSession\\\n","    .builder\\\n","    .config('spark.app.name', 'learning_spark_sql')\\\n","    .getOrCreate()"],"metadata":{"id":"tknww8B4PV95"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Read in the CSV file without any specific read options specified and show the top 10 rows."],"metadata":{"id":"mwzSNUpVPuZh"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","wiki_uniq_df = spark.read.csv('wiki_uniq_march_2022.csv')\n","\n","# show the first 10 rows\n","wiki_uniq_df.show(10, truncate = False)"],"metadata":{"id":"PsJeS6LNPwcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Read in the CSV file with an option to treat the first row as a header and show the top 10 rows."],"metadata":{"id":"n3sM86NoPyd3"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","wiki_uniq_w_header_df = spark.read.option('header',True).csv('wiki_uniq_march_2022.csv')\n","\n","# show the first 10 rows\n","wiki_uniq_w_header_df.show(10, truncate = False)"],"metadata":{"id":"e2JysStmQl8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Check the data types of `wiki_uniq_w_header_df`."],"metadata":{"id":"jjLWDUVQQpGh"}},{"cell_type":"code","source":["# show the data types\n","wiki_uniq_w_header_df.dtypes"],"metadata":{"id":"4nLDJGbGQq9n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Read in the CSV file with an option to treat the first row as a header and infer the schema. Then check the data types of `wiki_uniq_w_schema_df`."],"metadata":{"id":"rzXYhY7RQtjf"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","wiki_uniq_w_schema_df = spark.read.option('header',True).option('inferSchema', True).csv('wiki_uniq_march_2022.csv')\n","\n","# show the data types\n","wiki_uniq_w_schema_df.dtypes"],"metadata":{"id":"TDnT7_fqQvCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"J995p4oJQwy6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inspecting and Cleaning Data With PySpark"],"metadata":{"id":"weRx9yvRQzF2"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"learning_spark_sql\") \\\n","    .getOrCreate()\n","\n","# read in the Wikipedia unique visitors dataset\n","uniq_views_df = spark.read\\\n","    .option('header', True) \\\n","    .option('delimiter', ',') \\\n","    .option('inferSchema', True) \\\n","    .csv(\"wiki_uniq_march_2022.csv\")"],"metadata":{"id":"TpaKF8lVQ3dI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Print the DataFrame schema for `uniq_views_df`."],"metadata":{"id":"BH4i36SyQ59M"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","uniq_views_df.printSchema()"],"metadata":{"id":"5FC3xH2BQ75t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Show a description of the data for `uniq_views_df`."],"metadata":{"id":"qsgkE21QQ98H"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","uniq_views_df_desc = uniq_views_df.describe()\n","\n","# show summary\n","uniq_views_df_desc.show()"],"metadata":{"id":"Hl-0EsKRRLCY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Drop the columns `total_visitor_count` and `uniq_bot_visitors`."],"metadata":{"id":"S7nDZhjiROuR"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","uniq_counts_human_df = uniq_views_df.drop('total_visitor_count','uniq_bot_visitors')\n","\n","# show the first 5 rows\n","uniq_counts_human_df.show(5)"],"metadata":{"id":"Zzf1_pB1Rai7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Rename `uniq_human_visitors` to `unique_site_visitors`."],"metadata":{"id":"lh3KdCACRYW-"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","uniq_counts_final_df = uniq_counts_human_df.withColumnRenamed('uniq_human_visitors', 'unique_site_visitors')\n","\n","# show the first 5 rows\n","uniq_counts_final_df.show(5)"],"metadata":{"id":"YhMJNBobRc2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"dNv2FE4zRetH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Querying PySpark DataFrames"],"metadata":{"id":"U243Wk5JRhOw"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a New SparkSession\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"learning_spark_sql\") \\\n","    .getOrCreate()\n","\n","# Read in Wikipedia Unique Visitors Dataset\n","wiki_uniq_df = spark.read\\\n","    .option('header', True) \\\n","    .option('delimiter', ',') \\\n","    .option('inferSchema', True) \\\n","    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")"],"metadata":{"id":"kwQL1meMRjuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Filter the DataFrame to sites with `language_code` is `\"ar\"`."],"metadata":{"id":"Lly21dp1Roe_"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","ar_site_visitors = wiki_uniq_df\\\n","    .filter(wiki_uniq_df.language_code == 'ar')\n","\n","# show the DataFrame\n","ar_site_visitors.show()"],"metadata":{"id":"_CMoago7RqIu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Filter the DataFrame to sites with `language_code` is `\"ar\"` and keep only the columns `domain` and `uniq_human_visitors`."],"metadata":{"id":"HERVl5EORr1s"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","ar_visitors_slim = wiki_uniq_df\\\n","    .select(['domain', 'uniq_human_visitors'])\\\n","    .filter(wiki_uniq_df.language_code == 'ar')\n","\n","# show the DataFrame\n","ar_visitors_slim.show()"],"metadata":{"id":"bKqSfG5ZRuZU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Calculate the sum of all `uniq_human_visitors` grouped by `site_type` and ordered from highest to lowest page views."],"metadata":{"id":"3c51EvyzRwXv"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","top_visitors_site_type = wiki_uniq_df.select(['site_type', 'uniq_human_visitors'])\\\n","    .groupBy('site_type')\\\n","    .sum()\\\n","    .orderBy('sum(uniq_human_visitors)', ascending=False)\n","\n","# show the DataFrame\n","top_visitors_site_type.show()"],"metadata":{"id":"nPvz3lqFUYsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"I-rQPNoWUbXv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Querying PySpark with SQL"],"metadata":{"id":"Le9oLf4KUeuP"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a new SparkSession\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"learning_spark_sql\") \\\n","    .getOrCreate()\n","\n","# Read in Wikipedia Unique Visitors Dataset\n","wiki_uniq_df = spark.read\\\n","    .option('header', True) \\\n","    .option('delimiter', ',') \\\n","    .option('inferSchema', True) \\\n","    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")\n","\n","# Create a temporary view with the DataFrame\n","wiki_uniq_df\\\n","    .createOrReplaceTempView('uniq_visitors_march')"],"metadata":{"id":"_bzmU0bGUl6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Filter the DataFrame to sites where `language_code` is `\"ar\"`."],"metadata":{"id":"s1XupixXUrCN"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","ar_site_visitors_qry = \"\"\"\n","    SELECT * FROM uniq_visitors_march\n","    WHERE language_code = 'ar';\n","\"\"\"\n","\n","# show the DataFrame\n","spark\\\n","    .sql(ar_site_visitors_qry)\\\n","    .show(truncate=False)"],"metadata":{"id":"Lzty_YS7Usj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Filter the DataFrame to sites with `language_code` is `\"ar\"` and keep only the columns `domain` and `uniq_human_visitors`."],"metadata":{"id":"VRhI7Z5xU0Fj"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","ar_site_visitors_slim_qry = \"\"\"\n","    SELECT domain, uniq_human_visitors\n","    FROM uniq_visitors_march\n","    WHERE language_code = 'ar';\n","\"\"\"\n","\n","# show the DataFrame\n","spark\\\n","    .sql(ar_site_visitors_slim_qry)\\\n","    .show(truncate=False)"],"metadata":{"id":"LAw3b4dSU5Mz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Calculate the sum of all uniq_human_visitors grouped by site_type and ordered from highest to lowest uniq_human_visitors."],"metadata":{"id":"Sopj3nbVU11n"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","site_top_type_qry = \"\"\"\n","    SELECT site_type, SUM(uniq_human_visitors)\n","    FROM uniq_visitors_march\n","    GROUP BY site_type\n","    ORDER BY SUM(uniq_human_visitors) DESC;\n","\"\"\"\n","\n","# show the DataFrame\n","spark\\\n","    .sql(site_top_type_qry)\\\n","    .show(truncate=False)"],"metadata":{"id":"hckYeW9AU-DF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"9fv6nXm1VALf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving PySpark DataFrames"],"metadata":{"id":"u9lTzbr_VCli"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a new SparkSession\n","spark = SparkSession\\\n","    .builder\\\n","    .config('spark.app.name', 'learning_spark_sql')\\\n","    .getOrCreate()\n","\n","# Read in Wikipedia Unique Visitors Dataset\n","wiki_uniq_df = spark.read\\\n","    .option('header', True) \\\n","    .option('delimiter', ',') \\\n","    .option('inferSchema', True) \\\n","    .csv(\"wiki_uniq_march_2022.csv\")"],"metadata":{"id":"BWJb7_hnVEFr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Run the code to create a new DataFrame with only `domain` and `uniq_human_visitors`."],"metadata":{"id":"YeQuE68JVHn1"}},{"cell_type":"code","source":["# select only domain and uniq_human visitors\n","uniq_human_visitors_df = wiki_uniq_df\\\n","    .select('domain', 'uniq_human_visitors')\n","\n","# show the new DataFrame\n","uniq_human_visitors_df.show()"],"metadata":{"id":"RM2CIkzFVKqS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Save the new DataFrame as CSV files."],"metadata":{"id":"nOZB9VDCVMzE"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","uniq_human_visitors_df.write.csv(\"./results/csv/uniq_human_visitors/\"\n","                                 , mode=\"overwrite\")\n"],"metadata":{"id":"T-BNH0Q7VP4O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Save the new DataFrame as Parquet files."],"metadata":{"id":"uWvYskOkVSZ4"}},{"cell_type":"code","source":["## YOUR SOLUTION HERE ##\n","uniq_human_visitors_df.write.parquet(\"./results/pq/uniq_human_visitors/\"\n","                                     , mode=\"overwrite\")\n"],"metadata":{"id":"H2AZjZnuVUz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"71xXxZ-0VWOn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implimentation"],"metadata":{"id":"2jSKz_GEVYXX"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a new SparkSession\n","spark = SparkSession\\\n","    .builder\\\n","    .config('spark.app.name', 'learning_spark_sql')\\\n","    .getOrCreate()\n","\n","# Read in Wikipedia Unique Visitors Dataset\n","wiki_uniq_df = spark.read\\\n","    .option('header', True) \\\n","    .option('delimiter', ',') \\\n","    .option('inferSchema', True) \\\n","    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")"],"metadata":{"id":"oZiGJiPJVbFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clear the SparkSession cache and delete the underlying `sparkContext`\n","spark.stop()"],"metadata":{"id":"zFZVfZXUVeHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new DataFrame named `internal_clickstream`\n","internal_clickstream = clickstream.select([\"source_page\", \"target_page\", \"click_count\"])\\\n","                                  .filter(clickstream.link_category == 'link')\n","\n","# Display the first few rows of the DataFrame in the notebook\n","internal_clickstream.show(5, truncate=False)\n","\n","\n","internal_clickstream = clickstream\\\n","    .select([\"source_page\", \"target_page\", \"click_count\"])\\\n","    .filter(clickstream.link_category == 'link')\n","\n","# Display the first few rows of the DataFrame in the notebook\n","internal_clickstream.show(truncate=False)"],"metadata":{"id":"8Qim4A2gVhpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pyspark project\n"],"metadata":{"id":"M-3cvBGqWu8X"}},{"cell_type":"markdown","source":[" ## Analyzing Wikipedia Clickstream Data"],"metadata":{"id":"qjSiR0EUXCPG"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"id":"u_oflYnVXFer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new SparkSession and assign it to a variable named spark .\n","spark = SparkSession \\\n"," .builder \\\n"," .getOrCreate()\n"],"metadata":{"id":"hyNlQa0QXI1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Create an RDD from a list of sample clickstream counts and save it as\n","clickstream_counts_rdd .\n","\"\"\"\n","sample_clickstream_counts = [\n"," [\"other-search\", \"Hanging_Gardens_of_Babylon\", \"external\", 47000],  [\"other-empty\", \"Hanging_Gardens_of_Babylon\", \"external\", 34600],  [\"Wonders_of_the_World\", \"Hanging_Gardens_of_Babylon\", \"link\", 14000],  [\"Babylon\", \"Hanging_Gardens_of_Babylon\", \"link\", 2500] ]\n","clickstream_counts_rdd = spark.sparkContext.parallelize(\n"," sample_clickstream_counts\n",")\n"],"metadata":{"id":"07XrlgYuXOKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a DataFrame from the RDD of sample clickstream counts\n","clickstream_sample_df = clickstream_counts_rdd\\\n"," .toDF([\"source_page\", \"target_page\", \"link_category\", \"link_count\"])\n","# Display the DataFrame to the notebook\n","clickstream_sample_df.show(5, truncate=False)\n"],"metadata":{"id":"eak29agKXl_d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","\n","```\n","\n","source_page         |target_page               |link_category |link_ count\n","--------------------+--------------------------+--------------+------------\n","other-search        |Hanging_Gardens_of_Babylon|external      |47000\n","--------------------+--------------------------+--------------+------------\n","other-empty         |Hanging_Gardens_of_Babylon|external      |34600\n","--------------------+--------------------------+--------------+------------\n","Wonders_of_the_World|Hanging_Gardens_of_Babylon|link          |14000\n","--------------------+--------------------------+--------------+------------\n","Babylon             |Hanging_Gardens_of_Babylon|link          |2500\n","```"],"metadata":{"id":"UHgT2w_IYCo6"}},{"cell_type":"code","source":["# Read the target directory (`./cleaned/clickstream/`) into a DataFrame (`clicks tream`)\n","clickstream = spark.read \\\n"," .option('header', True) \\\n"," .option('delimiter', '\\t') \\\n"," .option('inferSchema', True) \\\n"," .csv(\"./cleaned/clickstream/\")\n","# Display the DataFrame to the notebook\n","clickstream.show(5, truncate=False)"],"metadata":{"id":"qTbH0gOtaBND"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["output:\n","\n","only showing top 5 rows\n","```\n","|referrer           |resource      |link_category|language_code|click_count|\n","+-------------------+--------------+-------------+-------------+-----------+\n","|Daniel_Day-Lewis   |Phantom_Thread|link         |en           |43190      |\n","+-------------------+--------------+-------------+-------------+-----------+\n","|other-internal     |Phantom_Thread|external     |en           |21683      |\n","+-------------------+--------------+-------------+-------------+-----------+\n","|other-empty        |Phantom_Thread|external     |en           |169532     |\n","+-------------------+--------------+-------------+-------------+-----------+\n","|90th_Academy_Awards|Phantom_Thread|link         |en           |40449      |\n","+-------------------+--------------+-------------+-------------+-----------+\n","|other-search       |Phantom_Thread|external     |en           |536940     |\n","+-------------------+--------------+-------------+-------------+-----------+\n","```\n","\n"],"metadata":{"id":"tTJQ4AHIaACK"}},{"cell_type":"code","source":["# Display the schema of the `clickstream` DataFrame to the notebook\n","clickstream.printSchema()"],"metadata":{"id":"Mc8VDIv5bClY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","```\n","root\n","|-- referrer: string (nullable = true)\n","|-- resource: string (nullable = true)\n","|-- link_category: string (nullable = true)\n","|-- language_code: string (nullable = true)\n","|-- click_count: integer (nullable = true)\n","```\n","\n"],"metadata":{"id":"Kr00y8MnbIvj"}},{"cell_type":"code","source":["# Drop language_code columns\n","clickstream = clickstream.drop(\"language_code\")\n","\n","\"\"\" Display the first few rows of the DataFrame and the new schema in the\n","notebook clickstream.show(5, truncate=False)\n","\"\"\"\n","clickstream.printSchema()"],"metadata":{"id":"1MbhUjhIbTNz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","\n","only showing top 5 rows\n","```\n","|referrer           |resource      |link_category|click_count|\n","+-------------------+--------------+-------------+-----------+\n","|Daniel_Day-Lewis   |Phantom_Thread|link         |43190      |\n","+-------------------+--------------+-------------+-----------+\n","|other-internal     |Phantom_Thread|external     |21683      |\n","+-------------------+--------------+-------------+-----------+\n","|other-empty        |Phantom_Thread|external     |169532     |\n","+-------------------+--------------+-------------+-----------+\n","|90th_Academy_Awards|Phantom_Thread|link         |40449      |\n","+-------------------+--------------+-------------+-----------+\n","|other-search       |Phantom_Thread|external     |536940     |\n","+-------------------+--------------+-------------+-----------+\n","```\n","\n","\n","\n","```\n","root\n","|-- referrer: string (nullable = true)\n","|-- resource: string (nullable = true)\n","|-- link_category: string (nullable = true)\n","|-- click_count: integer (nullable = true)\n","```\n","\n"],"metadata":{"id":"n7INEiosblpp"}},{"cell_type":"code","source":["# Rename `referrer` and `resource` to `source_page` and `target_page` clickstream = clickstream\\\n"," .withColumnRenamed(\"referrer\", \"source_page\")\\\n"," .withColumnRenamed(\"resource\", \"target_page\")\n","# Display the first few rows of the DataFrame and the new schema in the notebook clickstream.show(5, truncate=False)\n","clickstream.printSchema()\n","\n"],"metadata":{"id":"T9NNSw0Ab88Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","```\n","+-------------------+--------------+-------------+-----------+\n","|source_page        |target_page   |link_category|click_count|\n","+-------------------+--------------+-------------+-----------+\n","|Daniel_Day-Lewis   |Phantom_Thread|link         |43190      |\n","|other-internal     |Phantom_Thread|external     |21683      |\n","|other-empty        |Phantom_Thread|external     |169532     |\n","|90th_Academy_Awards|Phantom_Thread|link         |40449      |\n","|other-search       |Phantom_Thread|external     |536940     |\n","+-------------------+--------------+-------------+-----------+\n","```\n","only showing top 5 rows\n","\n","\n","\n","```\n","root\n","|-- source_page: string (nullable = true)\n","|-- target_page: string (nullable = true)\n","|-- link_category: string (nullable = true)\n","|-- click_count: integer (nullable = true)\n","```\n"],"metadata":{"id":"jIfD0PSDcRBK"}},{"cell_type":"code","source":["\"\"\"Create a temporary view in the metadata for this `SparkSession` to make the\n","data queryable with `sparkSession.sql()`\n","\"\"\"\n","clickstream.createOrReplaceTempView(\"clickstream\")"],"metadata":{"id":"8CyY2WKQdAx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"Filter the dataset to entries with Hanging_Gardens_of_Babylon as the\n","target_page and order the result by click_count using PySpark DataFrame methods.\n","\"\"\"\n","\n","clickstream\\\n"," .filter(clickstream.target_page == 'Hanging_Gardens_of_Babylon')\\\n"," .orderBy('click_count', ascending=False)\\\n"," .show(10, truncate=False)\n","\n"],"metadata":{"id":"KWo_c7TldXHv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","\n","```\n","+----------------------------------+--------------------------+--------------+-----------+\n","|source_page                       |target_page               |link_category |click_count|\n","+----------------------------------+--------------------------+--------------+-----------+\n","|other-search                      |Hanging_Gardens_of_Babylon|external      |47088      |\n","|other-empty                       |Hanging_Gardens_of_Babylon|external      |34619      |\n","|Wonders_of_the_World              |Hanging_Gardens_of_Babylon|link          |14668      |\n","|Seven_Wonders_of_the_Ancient_World|Hanging_Gardens_of_Babylon|link          |12296      |\n","+----------------------------------+--------------------------+--------------+-----------+\n","```\n","\n"],"metadata":{"id":"VX2o1j6ud1Y3"}},{"cell_type":"code","source":["# Perform the same analysis as the previous exercise using a SQL query.\n","# Filter and sort the DataFrame using SQL\n","spark.sql(\n"," \"\"\"\n"," SELECT *\n"," FROM clickstream\n"," WHERE target_page = 'Hanging_Gardens_of_Babylon'\n"," ORDER BY click_count DESC\n"," \"\"\"\n",").show(10, truncate=False)"],"metadata":{"id":"WebU4Yf6fTm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","\n","```\n","+-----------------------------------+---------------------------+---------------+------------+\n","|source_page                       |target_page                 |link_category  |click_count |\n","+-----------------------------------+---------------------------+---------------+------------+\n","|other-search                       |Hanging_Gardens_of_Babylon |external       |47088       |\n","|other-empty                        |Hanging_Gardens_of_Babylon |external       |34619       |\n","|Wonders_of_the_World               |Hanging_Gardens_of_Babylon |link           |14668       |\n","|Seven_Wonders_of_the_Ancient_World |Hanging_Gardens_of_Babylon |link           | 12296      |\n","+-----------------------------------+---------------------------+---------------+------------+\n","```\n","\n"],"metadata":{"id":"LgYWpuWQfbL6"}},{"cell_type":"code","source":["# Calculate the sum of click_count grouped by link_category using PySpark DataFrame methods.\n","# Aggregate the DataFrame using PySpark DataFrame Methods\n","clickstream\\\n"," .groupBy('link_category')\\\n"," .sum()\\\n"," .show(truncate=False)\n","\n"],"metadata":{"id":"qHbKMNtIhI4H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","\n","```\n","+-------------+----------------+\n","|link_category|sum(click_count)|\n","+-------------+----------------+\n","|link         |97805811        |\n","|other        |9338172         |\n","|external     |3248677856      |\n","+-------------+----------------+\n","```\n","\n","\n"],"metadata":{"id":"aVutxdv_hRex"}},{"cell_type":"code","source":["# Perform the same analysis as the previous exercise using a SQL query.\n","# Aggregate the DataFrame using SQL\n","spark.sql(\n"," \"\"\"\n"," SELECT link_category, SUM(click_count) FROM clickstream  GROUP BY link_category\n"," \"\"\"\n",").show(truncate=False)\n","\n"],"metadata":{"id":"cFYgsnEdhgjl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","\n","```\n","+-------------+----------------+\n","|link_category|sum(click_count)|\n","+-------------+----------------+\n","|link         |97805811        |\n","|other        |9338172         |\n","|external     |3248677856      |\n","+-------------+----------------+\n","```"],"metadata":{"id":"KbFO0pXBhltL"}},{"cell_type":"code","source":["\"\"\"Let's create a new DataFrame named internal_clickstream that only contains\n","article pairs where link_category is link .\n","\"\"\"\n","\n","# Create a new DataFrame (named `internal_clickstream`) using `filter` to select rows to\n","# a specific condition and `select` to choose which columns to return from the query.\n","internal_clickstream = clickstream\\\n"," .select([\"source_page\", \"target_page\", \"click_count\"])\\\n"," .filter(clickstream.link_category == 'link')\n","# Display the first few rows of the DataFrame in the notebook\n","internal_clickstream.show(truncate=False)"],"metadata":{"id":"7QQ403N_hzcR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Output:\n","\n","\n","```\n","+----------------------------+----------------------------+-----------+\n","|source_page                 |target_page                 |click_count|\n","+----------------------------+----------------------------+-----------+\n","|Daniel_Day-Lewis            |Phantom_Thread              |43190      |\n","|90th_Academy_Awards         |Phantom_Thread              |40449      |\n","|Shinee                      |Kim_Jong-hyun_(singer)      |24433      |\n","|Agnyaathavaasi              |Anu_Emmanuel                |15020      |\n","|Naa_Peru_Surya              |Anu_Emmanuel                |12361      |\n","|Mariah_Carey                |Nick_Cannon                 |16214      |\n","|Kesha                       |Rainbow_(Kesha_album)       |11448      |\n","|David_Attenborough          |John_Attenborough           |11252      |\n","|Boney_M.                    |Bobby_Farrell               |14095      |\n","|The_End_of_the_F***ing_World|Jessica_Barden              |237279     |\n","|Quentin_Tarantino           |The_Hateful_Eight           |12018      |\n","|Ready_Player_One_(film)     |Olivia_Cooke                |17468      |\n","|Royal_Rumble_(2018)         |Kevin_Owens_and_Sami_Zayn   |11503      |\n","|Macaulay_Culkin             |Brenda_Song                 |20477      |  \n","|Altered_Carbon              |Altered_Carbon_(TV_series)  |23962      |\n","|Lil_Pump                    |Smokepurpp                  |36736      |\n","|Fifth_Harmony               |Camila_Cabello              |30959      |\n","|Havana_(Camila_Cabello_song)|Camila_Cabello              |12803      |\n","|Jennifer_Aniston            |John_Aniston                |26498      |\n","|Kingsman:_The_Golden_Circle |Kingsman:_The_Secret_Service|11969      |\n","+----------------------------+----------------------------+-----------+\n","only showing top 20 rows\n","\n","```\n","\n"],"metadata":{"id":"uJtMp4EsiVAg"}},{"cell_type":"code","source":["# Save the `internal_clickstream` DataFrame to a series of CSV files in `./results/article_links_csv/`\n","# with `DataFrame.write.csv()`\n","internal_clickstream\\\n"," .write\\\n"," .csv('./results/article_links_csv/', mode=\"overwrite\")"],"metadata":{"id":"hVqFaMyrjk7a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Save the `internal_clickstream` DataFrame to a series of parquet files in `./results/article_links_parquet/`\n","# with `DataFrame.write.parquet()`\n","internal_clickstream\\\n"," .write\\\n"," .parquet('./results/article_links_parquet/', mode=\"overwrite\")"],"metadata":{"id":"xOKqN9yij1be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Close the SparkSession and underlying sparkContext . What happens if you we call clickstream.show() after closing the SparkSession ?\n","\n","# Stop the notebook's `SparkSession` and `SparkContext`\n","spark.stop()\n","\n","# The SparkSession and sparkContext are stopped; the following line will throw  # an error:\n","clickstream.show()"],"metadata":{"id":"DXPhJGl5j-Hc"},"execution_count":null,"outputs":[]}]}